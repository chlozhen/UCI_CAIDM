{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jarvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7fa0692b34a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjarvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjarvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjarvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjarvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimshow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jarvis'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from jarvis.train import datasets\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import tools as jtools\n",
    "from jarvis.utils.display import imshow\n",
    "from tensorflow.keras import Input, Model, models, layers, metrics\n",
    "from tensorflow import losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020-11-05 13:14:44 ] CUDA_VISIBLE_DEVICES automatically set to: 3           \n"
     ]
    }
   ],
   "source": [
    "from jarvis.utils.general import gpus\n",
    "gpus.autoselect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = jtools.get_paths('xr/breast-fgt')\n",
    "client = Client('/data/raw/xr_breast_fgt/data/ymls/client.yml')\n",
    "gen_train, gen_valid = client.create_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "# xs, ys = next(gen_train)\n",
    "\n",
    "# for key, arr in xs.items():\n",
    "#     print('xs key: {} | shape = {}'.format(key.ljust(8), arr.shape))\n",
    "# for key, arr in ys.items():\n",
    "#     print('ys key: {} | shape = {}'.format(key.ljust(8), arr.shape))\n",
    "\n",
    "# imshow(xs['dat'][0])\n",
    "# imshow(xs['dat'], figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = client.get_inputs(Input)\n",
    "\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "  #  'kernel_initializer': 'he_normal'}\n",
    "\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.LeakyReLU()(x)\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "\n",
    "# T1\n",
    "l1 = conv2(48, conv1(48, conv1(48, inputs['dat'])))\n",
    "l2 = conv2(56, conv1(56, conv1(56, l1)))\n",
    "l3 = conv2(64, conv1(64, conv1(64, l2)))\n",
    "l4 = conv2(80, conv1(80, conv1(80, l3)))\n",
    "l5 = conv2(96, conv1(96, conv1(96, l4)))\n",
    "l6 = conv2(112, conv1(112, conv1(112, l5)))\n",
    "l7 = conv2(128, conv1(128, conv1(128, l6)))\n",
    "f0 = layers.Reshape((1, 1, 1, 2 * 2 * 128))(l7)\n",
    "trial = 1\n",
    "\n",
    "# # # T2\n",
    "# l1 = conv2(48, conv1(48, inputs['dat']))\n",
    "# l2 = conv2(56, conv1(56, l1))\n",
    "# l3 = conv2(64, conv1(64, l2))\n",
    "# l4 = conv2(80, conv1(80, l3))\n",
    "# l5 = conv2(96, conv1(96, l4))\n",
    "# l6 = conv2(112, conv1(112, l5))\n",
    "# l7 = conv2(128, conv1(128, l6))\n",
    "# f0 = layers.Reshape((1, 1, 1, 2 * 2 * 128))(l7)\n",
    "# trial = 2\n",
    "\n",
    "# # T3\n",
    "# l1 = conv2(48, conv1(48, inputs['dat']))\n",
    "# l2 = conv2(56, conv1(56, l1))\n",
    "# l3 = conv2(64, conv1(64, l2))\n",
    "# l4 = conv2(80, conv1(80, l3))\n",
    "# l5 = conv2(96, conv1(96, l4))\n",
    "# l6 = conv2(112, conv1(112, l5))\n",
    "# l7 = conv2(128, conv1(128, l6))\n",
    "# l8 = conv2(256, conv1(256, l7))\n",
    "# f0 = layers.Reshape((1, 1, 1, 1 * 1 * 256))(l8)\n",
    "# trial = 3\n",
    "\n",
    "logits = {}\n",
    "logits['lbl'] = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), activation='sigmoid', name='lbl')(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=logits)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=5e-5),\n",
    "    loss={'lbl': losses.MeanAbsoluteError()}, metrics={'lbl':losses.MeanSquaredError()},\n",
    "    experimental_run_tf_function=False)\n",
    "\n",
    "# ***mean absolute error (MAE)\n",
    "# mean squared error (MSE)\n",
    "# Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Epoch 1/6\n",
      "500/500 [==============================] - 114s 229ms/step - loss: 0.0838 - mean_squared_error: 0.0154\n",
      "Epoch 2/6\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0622 - mean_squared_error: 0.0088WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Epoch 1/6\n",
      "500/500 [==============================] - 50s 101ms/step - loss: 0.0736 - mean_squared_error: 0.0135\n",
      "500/500 [==============================] - 154s 309ms/step - loss: 0.0621 - mean_squared_error: 0.0088 - val_loss: 0.0736 - val_mean_squared_error: 0.0135\n",
      "Epoch 3/6\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.0520 - mean_squared_error: 0.0064\n",
      "Epoch 4/6\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0049WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Epoch 1/6\n",
      "500/500 [==============================] - 49s 98ms/step - loss: 0.0635 - mean_squared_error: 0.0086\n",
      "500/500 [==============================] - 153s 305ms/step - loss: 0.0456 - mean_squared_error: 0.0049 - val_loss: 0.0635 - val_mean_squared_error: 0.0086\n",
      "Epoch 5/6\n",
      "500/500 [==============================] - 107s 214ms/step - loss: 0.0399 - mean_squared_error: 0.0036\n",
      "Epoch 6/6\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0029WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Epoch 1/6\n",
      "500/500 [==============================] - 49s 97ms/step - loss: 0.0667 - mean_squared_error: 0.0097\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.0362 - mean_squared_error: 0.0029 - val_loss: 0.0667 - val_mean_squared_error: 0.0097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7d503bfa90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=500, \n",
    "    epochs=6,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=500,\n",
    "    validation_freq=2,\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "#DATE       MODEL         EPOCH        RESULTS\n",
    "#11042020 - (SU OG)       4            loss: 0.0494 - mean_squared_error: 0.0060 - val_loss: 0.0643 - val_mean_squared_error: 0.0092\n",
    "#11052020 - (T1)          4            loss: 0.0436 - mean_squared_error: 0.0047 - val_loss: 0.0590 - val_mean_squared_error: 0.0082\n",
    "#11052020 - (T2)          4            loss: 0.0417 - mean_squared_error: 0.0043 - val_loss: 0.0613 - val_mean_squared_error: 0.0089\n",
    "#11052020 - (T3)          4            loss: 0.0506 - mean_squared_error: 0.0061 - val_loss: 0.0657 - val_mean_squared_error: 0.0099\n",
    "#11052020 - (T3)          6            loss: 0.0455 - mean_squared_error: 0.0049 - val_loss: 0.0626 - val_mean_squared_error: 0.0090\n",
    "#11052020 - (T2)          6            loss: 0.0344 - mean_squared_error: 0.0028 - val_loss: 0.0635 - val_mean_squared_error: 0.0100\n",
    "#11052020 - (T1)          6            loss: 0.0362 - mean_squared_error: 0.0029 - val_loss: 0.0667 - val_mean_squared_error: 0.0097\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_trial_{}.hdf5'.format(trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train, test_valid = client.create_generators(test=True)\n",
    "# xs, ys = next(test_valid)\n",
    "xs, ys = next(gen_valid)\n",
    "logits = model.predict(xs['dat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(a, b):\n",
    "    mse = ((a - b)**2).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #evaluation\n",
    "# model = models.load_model('model_11042020.hdf5', compile=False)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for x, y in gen_valid:\n",
    "#     # --- Predict Percentage\n",
    "#     logits = model.predict(x['dat'])\n",
    "#     if type(logits) is dict:\n",
    "#         logits = logits['lbl']\n",
    "        \n",
    "#     pred = logits\n",
    "#     trues = y['lbl']\n",
    "    \n",
    "#     loss = mse(trues, pred)\n",
    "# #     print('.', end='')\n",
    "#     losses.append(loss)\n",
    "\n",
    "# losses = np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
